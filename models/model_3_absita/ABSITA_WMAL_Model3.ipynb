{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ABSITA_WMAL_Model3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-sf82movkvc",
        "outputId": "a37cb975-22bf-4523-94c0-730dedc7fd14"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLlrzHzRv9XU"
      },
      "source": [
        "import joblib\n",
        "emb_matrix_train = joblib.load(\"/content/drive/MyDrive/absita_comfort_embedd_128_768_alberto_train.joblib\")\n",
        "wmal_matrix_train = joblib.load(\"/content/drive/MyDrive/wmal_train_comfort_128.joblib\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se9351L01RCX"
      },
      "source": [
        "import pandas as pd\n",
        "dataset_DF = pd.read_csv(\"/content/drive/MyDrive/comfort_train_processed.csv\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2deOEjg1W_e",
        "outputId": "eb012590-9b82-4be1-e9d1-39a9b95135f6"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# define example\n",
        "data = dataset_DF[\"pos\"]\n",
        "values = array(data)\n",
        "print(values)\n",
        "# integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(values)\n",
        "print(integer_encoded)\n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "print(onehot_encoded)\n",
        "# invert first example\n",
        "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
        "print(inverted)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 1 ... 1 0 0]\n",
            "[0 1 1 ... 1 0 0]\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " ...\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Tc64TlXwys4",
        "outputId": "95e4f55a-e1c1-46ce-b54a-5748bf71cfc8"
      },
      "source": [
        "from keras.layers import Input, Dense, LSTM, Bidirectional, TimeDistributed, Flatten, Concatenate\n",
        "!pip install keras-self-attention"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.7/dist-packages (0.51.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOySvAIz3XeS",
        "outputId": "d9d6fa54-e10d-4e32-c97c-7b1b42fdb282"
      },
      "source": [
        "!pip install keras-self-attention\n",
        "from keras_self_attention import SeqWeightedAttention, SeqSelfAttention\n",
        "from keras.layers import Add, Multiply"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.7/dist-packages (0.51.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaSsX9AewjPU"
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "input_emb = Input(shape=(128,768))\n",
        "input_wmal = Input(shape=(128,))\n",
        "\n",
        "#Bi-LSTM\n",
        "#bi = Bidirectional(LSTM(32, return_sequences=True))(input_emb)\n",
        "bi = LSTM(64, return_sequences=True)(input_emb)\n",
        "#att = SeqWeightedAttention()(bi)\n",
        "att = SeqSelfAttention(attention_width=15,attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
        "                       attention_activation=\"sigmoid\",\n",
        "                       kernel_regularizer=keras.regularizers.l2(1e-8),#(1e-6),\n",
        "                       bias_regularizer=keras.regularizers.l1(1e-8),#(1e-4),\n",
        "                       attention_regularizer_weight=1e-8,#1e-4,\n",
        "                       use_attention_bias=True,return_attention=False) (bi)\n",
        "\n",
        "app = Multiply() ([bi, att])                      \n",
        "flat = Flatten()(app)\n",
        "\n",
        "#WMAL \n",
        "d1 = Dense(64) (input_wmal)\n",
        "\n",
        "#Concat\n",
        "concat = Concatenate(axis=1)([flat, d1])\n",
        "\n",
        "d2 = Dense(32) (concat)\n",
        "output = Dense(2,activation=\"softmax\") (d2)\n",
        "\n",
        "m = keras.Model(inputs=[input_emb,input_wmal], outputs=output)\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.00005)\n",
        "m.compile(loss=\"categorical_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzpaI6nT3pSw"
      },
      "source": [
        "m.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c__iZWCz32Tv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bc5e36d-df4a-4c3a-c4c6-e79b3b60a238"
      },
      "source": [
        "m.fit([emb_matrix_train,wmal_matrix_train],onehot_encoded,batch_size=128, epochs=10,validation_split=0.15)\n",
        "#m.load_weights(\"/content/drive/MyDrive/SI_AI_KBS/integrazione_AlBERTo_WMAL/sentipolc_agritrend/models/model_3_sentipolc/model_3_pos_sentipolc_0.73645.h5\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "18/18 [==============================] - 5s 93ms/step - loss: 0.6569 - accuracy: 0.6514 - val_loss: 0.5958 - val_accuracy: 0.7731\n",
            "Epoch 2/10\n",
            "18/18 [==============================] - 1s 45ms/step - loss: 0.5821 - accuracy: 0.7770 - val_loss: 0.5067 - val_accuracy: 0.8479\n",
            "Epoch 3/10\n",
            "18/18 [==============================] - 1s 44ms/step - loss: 0.4791 - accuracy: 0.8647 - val_loss: 0.3986 - val_accuracy: 0.9002\n",
            "Epoch 4/10\n",
            "18/18 [==============================] - 1s 58ms/step - loss: 0.3798 - accuracy: 0.8841 - val_loss: 0.3170 - val_accuracy: 0.9002\n",
            "Epoch 5/10\n",
            "18/18 [==============================] - 1s 63ms/step - loss: 0.3169 - accuracy: 0.8995 - val_loss: 0.2761 - val_accuracy: 0.9077\n",
            "Epoch 6/10\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 0.2745 - accuracy: 0.9123 - val_loss: 0.2528 - val_accuracy: 0.9177\n",
            "Epoch 7/10\n",
            "18/18 [==============================] - 1s 61ms/step - loss: 0.2465 - accuracy: 0.9264 - val_loss: 0.2430 - val_accuracy: 0.9202\n",
            "Epoch 8/10\n",
            "18/18 [==============================] - 1s 64ms/step - loss: 0.2239 - accuracy: 0.9374 - val_loss: 0.2332 - val_accuracy: 0.9177\n",
            "Epoch 9/10\n",
            "18/18 [==============================] - 2s 90ms/step - loss: 0.2053 - accuracy: 0.9409 - val_loss: 0.2246 - val_accuracy: 0.9252\n",
            "Epoch 10/10\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.1880 - accuracy: 0.9445 - val_loss: 0.2216 - val_accuracy: 0.9277\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd9f07a3c50>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEm3BOiY8EHn"
      },
      "source": [
        "emb_matrix_test = joblib.load(\"/content/drive/MyDrive/absita_comfort_embedd_128_768_alberto_test.joblib\")\n",
        "dataset_test = pd.read_csv(\"/content/drive/MyDrive/comfort_test_processed.csv\")\n",
        "wmal_matrix_test = joblib.load(\"/content/drive/MyDrive/wmal_test_comfort_128.joblib\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ictcNZPP8TWc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9985246-9feb-43df-f413-05eb5fdb05a8"
      },
      "source": [
        "# define example\n",
        "data = dataset_test[\"pos\"]\n",
        "values = array(data)\n",
        "print(values)\n",
        "# integer encode\n",
        "integer_encoded_test = label_encoder.transform(values)\n",
        "print(integer_encoded_test)\n",
        "# binary encode\n",
        "integer_encoded_test = integer_encoded_test.reshape(len(integer_encoded_test), 1)\n",
        "onehot_encoded_test = onehot_encoder.transform(integer_encoded_test)\n",
        "print(onehot_encoded_test)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 ... 1 1 0]\n",
            "[0 0 0 ... 1 1 0]\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " ...\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnGfTDKQ8tFG"
      },
      "source": [
        "predictions = m.predict([emb_matrix_test,wmal_matrix_test])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24u9a3jU87UC",
        "outputId": "a95abd36-d29c-4b07-d568-5f5ff33f759d"
      },
      "source": [
        "print(predictions)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.7571837  0.24281628]\n",
            " [0.7571837  0.24281628]\n",
            " [0.99683666 0.00316327]\n",
            " ...\n",
            " [0.05369868 0.9463013 ]\n",
            " [0.00298287 0.99701715]\n",
            " [0.9427493  0.05725071]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UG-0trQ9LMV"
      },
      "source": [
        "import numpy as np\n",
        "predictions_vec = np.zeros((1171,4))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC7taZDs9Y-_"
      },
      "source": [
        "i = 0\n",
        "confidences= []\n",
        "for item in predictions:\n",
        "  index = np.argmax(item);\n",
        "  confidences.append(item[index])\n",
        "  predictions_vec[i][index] = 1\n",
        "  i = i+1"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs_Wp1cA9opa"
      },
      "source": [
        "labels_to_eval = []\n",
        "for vec in predictions_vec:\n",
        "  inverted = label_encoder.inverse_transform([argmax(vec)])[0]\n",
        "  labels_to_eval.append(inverted)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao9u_d0CEUC-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ae98c3-80c4-47e1-eb2f-2f4b8ac63fde"
      },
      "source": [
        "confidences"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7571837,\n",
              " 0.7571837,\n",
              " 0.99683666,\n",
              " 0.99683666,\n",
              " 0.7450328,\n",
              " 0.9859881,\n",
              " 0.87445587,\n",
              " 0.85536635,\n",
              " 0.54273385,\n",
              " 0.83280396,\n",
              " 0.9995382,\n",
              " 0.9995382,\n",
              " 0.83800995,\n",
              " 0.831912,\n",
              " 0.97995687,\n",
              " 0.98838717,\n",
              " 0.98838717,\n",
              " 0.98838717,\n",
              " 0.87874264,\n",
              " 0.99830365,\n",
              " 0.55009407,\n",
              " 0.9736483,\n",
              " 0.65434664,\n",
              " 0.99985754,\n",
              " 0.5114623,\n",
              " 0.95815194,\n",
              " 0.7034773,\n",
              " 0.86372817,\n",
              " 0.86372817,\n",
              " 0.82412875,\n",
              " 0.7754234,\n",
              " 0.97692966,\n",
              " 0.9952905,\n",
              " 0.9952905,\n",
              " 0.7851796,\n",
              " 0.97738487,\n",
              " 0.9965184,\n",
              " 0.9771577,\n",
              " 0.79009485,\n",
              " 0.9495435,\n",
              " 0.8851659,\n",
              " 0.92479414,\n",
              " 0.9147471,\n",
              " 0.8859923,\n",
              " 0.961898,\n",
              " 0.5937947,\n",
              " 0.9921989,\n",
              " 0.8398144,\n",
              " 0.991783,\n",
              " 0.73881173,\n",
              " 0.72593194,\n",
              " 0.99472225,\n",
              " 0.99472225,\n",
              " 0.70022327,\n",
              " 0.86524194,\n",
              " 0.89421797,\n",
              " 0.9553152,\n",
              " 0.9946498,\n",
              " 0.8955826,\n",
              " 0.97116226,\n",
              " 0.910455,\n",
              " 0.99949145,\n",
              " 0.9965199,\n",
              " 0.9965199,\n",
              " 0.9424835,\n",
              " 0.99058163,\n",
              " 0.8550585,\n",
              " 0.69613194,\n",
              " 0.995082,\n",
              " 0.995082,\n",
              " 0.995082,\n",
              " 0.995082,\n",
              " 0.7483635,\n",
              " 0.9884545,\n",
              " 0.9494327,\n",
              " 0.69409555,\n",
              " 0.6754982,\n",
              " 0.7559037,\n",
              " 0.7246905,\n",
              " 0.9215431,\n",
              " 0.9887911,\n",
              " 0.6691174,\n",
              " 0.6965799,\n",
              " 0.77113134,\n",
              " 0.88996357,\n",
              " 0.96620077,\n",
              " 0.98723537,\n",
              " 0.9347651,\n",
              " 0.69463325,\n",
              " 0.9189611,\n",
              " 0.5960675,\n",
              " 0.94839984,\n",
              " 0.9854247,\n",
              " 0.82033396,\n",
              " 0.9805431,\n",
              " 0.9295393,\n",
              " 0.9701607,\n",
              " 0.5005803,\n",
              " 0.9942264,\n",
              " 0.80393976,\n",
              " 0.80393976,\n",
              " 0.9938314,\n",
              " 0.99070823,\n",
              " 0.99070823,\n",
              " 0.986662,\n",
              " 0.7507549,\n",
              " 0.99971026,\n",
              " 0.99971026,\n",
              " 0.9298619,\n",
              " 0.8921644,\n",
              " 0.7242897,\n",
              " 0.97107273,\n",
              " 0.88050544,\n",
              " 0.99482673,\n",
              " 0.88540804,\n",
              " 0.88540804,\n",
              " 0.98809916,\n",
              " 0.98809916,\n",
              " 0.763429,\n",
              " 0.9243602,\n",
              " 0.98985964,\n",
              " 0.9931485,\n",
              " 0.99807596,\n",
              " 0.964606,\n",
              " 0.99805593,\n",
              " 0.5043737,\n",
              " 0.80061454,\n",
              " 0.66632646,\n",
              " 0.8717815,\n",
              " 0.7433452,\n",
              " 0.9952043,\n",
              " 0.9952043,\n",
              " 0.7783427,\n",
              " 0.94323176,\n",
              " 0.9422245,\n",
              " 0.7167762,\n",
              " 0.9780432,\n",
              " 0.9780432,\n",
              " 0.97234994,\n",
              " 0.9883471,\n",
              " 0.89618933,\n",
              " 0.9669959,\n",
              " 0.9291903,\n",
              " 0.8298223,\n",
              " 0.9978477,\n",
              " 0.9869516,\n",
              " 0.9689854,\n",
              " 0.54868305,\n",
              " 0.9803463,\n",
              " 0.98292667,\n",
              " 0.9392424,\n",
              " 0.9823032,\n",
              " 0.9990728,\n",
              " 0.9990728,\n",
              " 0.9692434,\n",
              " 0.9948731,\n",
              " 0.5725482,\n",
              " 0.96549946,\n",
              " 0.8982433,\n",
              " 0.99664575,\n",
              " 0.8646779,\n",
              " 0.9987987,\n",
              " 0.99242014,\n",
              " 0.7540624,\n",
              " 0.9560795,\n",
              " 0.99483585,\n",
              " 0.607131,\n",
              " 0.9983175,\n",
              " 0.85951126,\n",
              " 0.9797197,\n",
              " 0.9797197,\n",
              " 0.54559195,\n",
              " 0.84291285,\n",
              " 0.9800679,\n",
              " 0.9496502,\n",
              " 0.89169925,\n",
              " 0.89169925,\n",
              " 0.95258856,\n",
              " 0.6588709,\n",
              " 0.67342126,\n",
              " 0.74866104,\n",
              " 0.9259095,\n",
              " 0.789772,\n",
              " 0.9988003,\n",
              " 0.7420988,\n",
              " 0.99428153,\n",
              " 0.987247,\n",
              " 0.7924891,\n",
              " 0.7924891,\n",
              " 0.8329616,\n",
              " 0.993511,\n",
              " 0.9996402,\n",
              " 0.89535403,\n",
              " 0.89535403,\n",
              " 0.8189681,\n",
              " 0.98160434,\n",
              " 0.98796356,\n",
              " 0.9070547,\n",
              " 0.98742396,\n",
              " 0.6977061,\n",
              " 0.95463884,\n",
              " 0.86422217,\n",
              " 0.88668966,\n",
              " 0.86344486,\n",
              " 0.81415004,\n",
              " 0.87526685,\n",
              " 0.9612072,\n",
              " 0.9714256,\n",
              " 0.9851799,\n",
              " 0.9020256,\n",
              " 0.99987566,\n",
              " 0.99987566,\n",
              " 0.91181296,\n",
              " 0.91181296,\n",
              " 0.96300685,\n",
              " 0.9909603,\n",
              " 0.9563742,\n",
              " 0.8742174,\n",
              " 0.9945716,\n",
              " 0.96673554,\n",
              " 0.93753576,\n",
              " 0.72561836,\n",
              " 0.999897,\n",
              " 0.64648676,\n",
              " 0.9975701,\n",
              " 0.9429658,\n",
              " 0.7786108,\n",
              " 0.6066471,\n",
              " 0.98894614,\n",
              " 0.72434175,\n",
              " 0.96120775,\n",
              " 0.89302987,\n",
              " 0.8962239,\n",
              " 0.86094606,\n",
              " 0.99642414,\n",
              " 0.8970389,\n",
              " 0.95658535,\n",
              " 0.9945214,\n",
              " 0.9744193,\n",
              " 0.99681515,\n",
              " 0.99681515,\n",
              " 0.72172534,\n",
              " 0.5482357,\n",
              " 0.99981135,\n",
              " 0.5795076,\n",
              " 0.60824203,\n",
              " 0.9734405,\n",
              " 0.9897161,\n",
              " 0.93855107,\n",
              " 0.77155596,\n",
              " 0.7494119,\n",
              " 0.8771277,\n",
              " 0.65121776,\n",
              " 0.8304095,\n",
              " 0.7475574,\n",
              " 0.9123602,\n",
              " 0.99382555,\n",
              " 0.99382555,\n",
              " 0.88248724,\n",
              " 0.8926889,\n",
              " 0.6050518,\n",
              " 0.71233034,\n",
              " 0.9460326,\n",
              " 0.9460326,\n",
              " 0.9460326,\n",
              " 0.9460326,\n",
              " 0.74932426,\n",
              " 0.58683765,\n",
              " 0.7413577,\n",
              " 0.9271629,\n",
              " 0.870363,\n",
              " 0.97009003,\n",
              " 0.99205726,\n",
              " 0.96465474,\n",
              " 0.7978055,\n",
              " 0.99790514,\n",
              " 0.53813237,\n",
              " 0.96948963,\n",
              " 0.63115436,\n",
              " 0.9938095,\n",
              " 0.90645194,\n",
              " 0.55711794,\n",
              " 0.94205415,\n",
              " 0.9847681,\n",
              " 0.78519624,\n",
              " 0.99089843,\n",
              " 0.8688155,\n",
              " 0.7691866,\n",
              " 0.73760915,\n",
              " 0.8550219,\n",
              " 0.99740857,\n",
              " 0.9991689,\n",
              " 0.9991689,\n",
              " 0.9991689,\n",
              " 0.99966156,\n",
              " 0.95245105,\n",
              " 0.99817216,\n",
              " 0.99817216,\n",
              " 0.9572115,\n",
              " 0.9572115,\n",
              " 0.99729997,\n",
              " 0.8608778,\n",
              " 0.9969843,\n",
              " 0.9969843,\n",
              " 0.6563546,\n",
              " 0.91015553,\n",
              " 0.60266566,\n",
              " 0.99200493,\n",
              " 0.8174605,\n",
              " 0.99693763,\n",
              " 0.99693763,\n",
              " 0.99693763,\n",
              " 0.99693763,\n",
              " 0.99900264,\n",
              " 0.895548,\n",
              " 0.998097,\n",
              " 0.8927091,\n",
              " 0.5829449,\n",
              " 0.5829449,\n",
              " 0.507718,\n",
              " 0.99117744,\n",
              " 0.99762243,\n",
              " 0.9963187,\n",
              " 0.7544503,\n",
              " 0.97508824,\n",
              " 0.97508824,\n",
              " 0.97508824,\n",
              " 0.97508824,\n",
              " 0.9288134,\n",
              " 0.9288134,\n",
              " 0.88139135,\n",
              " 0.85878944,\n",
              " 0.6823807,\n",
              " 0.9886537,\n",
              " 0.9886537,\n",
              " 0.9436653,\n",
              " 0.97264457,\n",
              " 0.951072,\n",
              " 0.974883,\n",
              " 0.99428093,\n",
              " 0.99428093,\n",
              " 0.9850583,\n",
              " 0.9850583,\n",
              " 0.9745841,\n",
              " 0.94208324,\n",
              " 0.9934569,\n",
              " 0.999884,\n",
              " 0.999884,\n",
              " 0.999884,\n",
              " 0.9369269,\n",
              " 0.9921003,\n",
              " 0.9921003,\n",
              " 0.55095416,\n",
              " 0.67258376,\n",
              " 0.80375403,\n",
              " 0.80375403,\n",
              " 0.80375403,\n",
              " 0.6889566,\n",
              " 0.994643,\n",
              " 0.99877924,\n",
              " 0.99877924,\n",
              " 0.99877924,\n",
              " 0.99877924,\n",
              " 0.8791782,\n",
              " 0.9979194,\n",
              " 0.9979194,\n",
              " 0.7307448,\n",
              " 0.98608714,\n",
              " 0.98608714,\n",
              " 0.98608714,\n",
              " 0.9660573,\n",
              " 0.99941707,\n",
              " 0.7690211,\n",
              " 0.7690211,\n",
              " 0.7690211,\n",
              " 0.97713304,\n",
              " 0.9065923,\n",
              " 0.99886405,\n",
              " 0.99886405,\n",
              " 0.99886405,\n",
              " 0.99886405,\n",
              " 0.99886405,\n",
              " 0.9427011,\n",
              " 0.99916077,\n",
              " 0.8617529,\n",
              " 0.7694706,\n",
              " 0.65783614,\n",
              " 0.93870795,\n",
              " 0.7611998,\n",
              " 0.7611998,\n",
              " 0.9894168,\n",
              " 0.9995296,\n",
              " 0.5049229,\n",
              " 0.97226906,\n",
              " 0.7163673,\n",
              " 0.9944675,\n",
              " 0.9944675,\n",
              " 0.9944675,\n",
              " 0.9944675,\n",
              " 0.99978226,\n",
              " 0.99978226,\n",
              " 0.9996376,\n",
              " 0.9996376,\n",
              " 0.98389536,\n",
              " 0.5121611,\n",
              " 0.6858264,\n",
              " 0.9889924,\n",
              " 0.97279817,\n",
              " 0.9627566,\n",
              " 0.98831207,\n",
              " 0.9831818,\n",
              " 0.61197376,\n",
              " 0.7876959,\n",
              " 0.9971053,\n",
              " 0.51381356,\n",
              " 0.6365647,\n",
              " 0.8653243,\n",
              " 0.86731714,\n",
              " 0.9944384,\n",
              " 0.99010086,\n",
              " 0.7417119,\n",
              " 0.92878944,\n",
              " 0.8377402,\n",
              " 0.7788655,\n",
              " 0.9850095,\n",
              " 0.99892694,\n",
              " 0.99892694,\n",
              " 0.99892694,\n",
              " 0.99892694,\n",
              " 0.98697203,\n",
              " 0.9976458,\n",
              " 0.6507144,\n",
              " 0.9216118,\n",
              " 0.93621236,\n",
              " 0.9809545,\n",
              " 0.9910806,\n",
              " 0.9910806,\n",
              " 0.6437552,\n",
              " 0.54972565,\n",
              " 0.74886286,\n",
              " 0.83686155,\n",
              " 0.9244873,\n",
              " 0.96320677,\n",
              " 0.99738806,\n",
              " 0.6818047,\n",
              " 0.9929349,\n",
              " 0.9112129,\n",
              " 0.89634395,\n",
              " 0.7158749,\n",
              " 0.9921634,\n",
              " 0.91489655,\n",
              " 0.99186337,\n",
              " 0.9138153,\n",
              " 0.74700725,\n",
              " 0.72193104,\n",
              " 0.994928,\n",
              " 0.994928,\n",
              " 0.9365049,\n",
              " 0.99949574,\n",
              " 0.9189094,\n",
              " 0.95708555,\n",
              " 0.72626024,\n",
              " 0.940829,\n",
              " 0.99452716,\n",
              " 0.9985592,\n",
              " 0.9870761,\n",
              " 0.9996376,\n",
              " 0.95357937,\n",
              " 0.7036671,\n",
              " 0.96701103,\n",
              " 0.7977599,\n",
              " 0.95626867,\n",
              " 0.99775606,\n",
              " 0.9497711,\n",
              " 0.771831,\n",
              " 0.9831881,\n",
              " 0.64123195,\n",
              " 0.987219,\n",
              " 0.8531534,\n",
              " 0.94196755,\n",
              " 0.88087356,\n",
              " 0.9982857,\n",
              " 0.99548376,\n",
              " 0.82188517,\n",
              " 0.88033783,\n",
              " 0.7746398,\n",
              " 0.9950387,\n",
              " 0.9950387,\n",
              " 0.93228114,\n",
              " 0.9590227,\n",
              " 0.9590227,\n",
              " 0.9833188,\n",
              " 0.9833188,\n",
              " 0.9997813,\n",
              " 0.9997813,\n",
              " 0.9985092,\n",
              " 0.9987748,\n",
              " 0.6878353,\n",
              " 0.9641868,\n",
              " 0.9641868,\n",
              " 0.9641868,\n",
              " 0.9425315,\n",
              " 0.93553483,\n",
              " 0.993511,\n",
              " 0.860004,\n",
              " 0.9961063,\n",
              " 0.860004,\n",
              " 0.860004,\n",
              " 0.9985551,\n",
              " 0.9969627,\n",
              " 0.98822135,\n",
              " 0.9203317,\n",
              " 0.9809498,\n",
              " 0.9552866,\n",
              " 0.9912286,\n",
              " 0.97841454,\n",
              " 0.8880496,\n",
              " 0.9432662,\n",
              " 0.9683023,\n",
              " 0.9683023,\n",
              " 0.93265676,\n",
              " 0.85828674,\n",
              " 0.81210536,\n",
              " 0.84207374,\n",
              " 0.6876993,\n",
              " 0.6876993,\n",
              " 0.6876993,\n",
              " 0.66689265,\n",
              " 0.84948444,\n",
              " 0.9998325,\n",
              " 0.9998325,\n",
              " 0.9998325,\n",
              " 0.99903286,\n",
              " 0.99903286,\n",
              " 0.99903286,\n",
              " 0.9848706,\n",
              " 0.9986645,\n",
              " 0.7474083,\n",
              " 0.90802646,\n",
              " 0.92194116,\n",
              " 0.7033845,\n",
              " 0.99779546,\n",
              " 0.9890771,\n",
              " 0.6159546,\n",
              " 0.9627286,\n",
              " 0.5764972,\n",
              " 0.7324969,\n",
              " 0.7903968,\n",
              " 0.95667547,\n",
              " 0.99493414,\n",
              " 0.8923434,\n",
              " 0.9519608,\n",
              " 0.94586945,\n",
              " 0.7261392,\n",
              " 0.7736231,\n",
              " 0.7777648,\n",
              " 0.8454642,\n",
              " 0.586849,\n",
              " 0.98858696,\n",
              " 0.96554714,\n",
              " 0.60751504,\n",
              " 0.99817896,\n",
              " 0.7004647,\n",
              " 0.9787732,\n",
              " 0.989965,\n",
              " 0.60130435,\n",
              " 0.99649125,\n",
              " 0.95306116,\n",
              " 0.9925212,\n",
              " 0.51639503,\n",
              " 0.87550414,\n",
              " 0.9888841,\n",
              " 0.9588036,\n",
              " 0.76594263,\n",
              " 0.60273135,\n",
              " 0.6026147,\n",
              " 0.9989925,\n",
              " 0.9853055,\n",
              " 0.99872833,\n",
              " 0.99872833,\n",
              " 0.99872833,\n",
              " 0.99872833,\n",
              " 0.99872833,\n",
              " 0.9343011,\n",
              " 0.9499338,\n",
              " 0.9938829,\n",
              " 0.975524,\n",
              " 0.9909366,\n",
              " 0.9819288,\n",
              " 0.9910255,\n",
              " 0.98650175,\n",
              " 0.784692,\n",
              " 0.8502125,\n",
              " 0.66110665,\n",
              " 0.98951095,\n",
              " 0.9914055,\n",
              " 0.9963426,\n",
              " 0.96125466,\n",
              " 0.9997316,\n",
              " 0.9997316,\n",
              " 0.8107299,\n",
              " 0.53574705,\n",
              " 0.83207035,\n",
              " 0.9624164,\n",
              " 0.9956921,\n",
              " 0.83889353,\n",
              " 0.96353745,\n",
              " 0.84399694,\n",
              " 0.943772,\n",
              " 0.93671596,\n",
              " 0.87405664,\n",
              " 0.60854936,\n",
              " 0.60854936,\n",
              " 0.91665435,\n",
              " 0.8817652,\n",
              " 0.9751065,\n",
              " 0.9914737,\n",
              " 0.85664755,\n",
              " 0.85664755,\n",
              " 0.9558448,\n",
              " 0.9951639,\n",
              " 0.9913645,\n",
              " 0.9913645,\n",
              " 0.7338595,\n",
              " 0.7176821,\n",
              " 0.9995442,\n",
              " 0.9463013,\n",
              " 0.8825887,\n",
              " 0.9995041,\n",
              " 0.9959347,\n",
              " 0.63906884,\n",
              " 0.9987748,\n",
              " 0.93495613,\n",
              " 0.93495613,\n",
              " 0.8471652,\n",
              " 0.9848556,\n",
              " 0.7817443,\n",
              " 0.7817443,\n",
              " 0.997267,\n",
              " 0.997267,\n",
              " 0.997267,\n",
              " 0.997267,\n",
              " 0.997267,\n",
              " 0.8561745,\n",
              " 0.9998456,\n",
              " 0.9998456,\n",
              " 0.54972565,\n",
              " 0.74886286,\n",
              " 0.74886286,\n",
              " 0.99885476,\n",
              " 0.74886286,\n",
              " 0.99933714,\n",
              " 0.95164144,\n",
              " 0.93861806,\n",
              " 0.98956394,\n",
              " 0.9883422,\n",
              " 0.98513293,\n",
              " 0.98198307,\n",
              " 0.8926244,\n",
              " 0.5290838,\n",
              " 0.8716276,\n",
              " 0.937302,\n",
              " 0.987468,\n",
              " 0.99831957,\n",
              " 0.9994473,\n",
              " 0.9994473,\n",
              " 0.93009347,\n",
              " 0.93009347,\n",
              " 0.99685115,\n",
              " 0.99685115,\n",
              " 0.9311691,\n",
              " 0.999485,\n",
              " 0.69489837,\n",
              " 0.944037,\n",
              " 0.78874767,\n",
              " 0.6192334,\n",
              " 0.6192334,\n",
              " 0.8781832,\n",
              " 0.7651848,\n",
              " 0.6349111,\n",
              " 0.99379,\n",
              " 0.78568906,\n",
              " 0.99227446,\n",
              " 0.99740595,\n",
              " 0.9994209,\n",
              " 0.9994209,\n",
              " 0.9994209,\n",
              " 0.55023795,\n",
              " 0.997505,\n",
              " 0.98284334,\n",
              " 0.81071377,\n",
              " 0.9898942,\n",
              " 0.9976452,\n",
              " 0.7279076,\n",
              " 0.7279076,\n",
              " 0.7279076,\n",
              " 0.99424344,\n",
              " 0.8762782,\n",
              " 0.8847008,\n",
              " 0.7301954,\n",
              " 0.98060876,\n",
              " 0.98060876,\n",
              " 0.98060876,\n",
              " 0.98794997,\n",
              " 0.98794997,\n",
              " 0.9803962,\n",
              " 0.89914584,\n",
              " 0.99338526,\n",
              " 0.9467566,\n",
              " 0.9511605,\n",
              " 0.98988616,\n",
              " 0.73808336,\n",
              " 0.6195207,\n",
              " 0.9539281,\n",
              " 0.9539281,\n",
              " 0.99835086,\n",
              " 0.99835086,\n",
              " 0.63003457,\n",
              " 0.63003457,\n",
              " 0.63003457,\n",
              " 0.63003457,\n",
              " 0.9897099,\n",
              " 0.67882276,\n",
              " 0.6125802,\n",
              " 0.9955973,\n",
              " 0.9653154,\n",
              " 0.9653154,\n",
              " 0.5553721,\n",
              " 0.9887341,\n",
              " 0.97907203,\n",
              " 0.9993529,\n",
              " 0.9993529,\n",
              " 0.9993529,\n",
              " 0.9990907,\n",
              " 0.9990907,\n",
              " 0.97386575,\n",
              " 0.7995937,\n",
              " 0.7891729,\n",
              " 0.9532298,\n",
              " 0.9532298,\n",
              " 0.9532298,\n",
              " 0.72407323,\n",
              " 0.99425113,\n",
              " 0.96604806,\n",
              " 0.96604806,\n",
              " 0.96604806,\n",
              " 0.9935163,\n",
              " 0.8824414,\n",
              " 0.9599479,\n",
              " 0.5998035,\n",
              " 0.5998035,\n",
              " 0.5998035,\n",
              " 0.9375692,\n",
              " 0.5338752,\n",
              " 0.9546996,\n",
              " 0.63252765,\n",
              " 0.9987453,\n",
              " 0.98603034,\n",
              " 0.8806165,\n",
              " 0.91564035,\n",
              " 0.893822,\n",
              " 0.61979693,\n",
              " 0.61979693,\n",
              " 0.8412929,\n",
              " 0.5844063,\n",
              " 0.72129667,\n",
              " 0.66725695,\n",
              " 0.66725695,\n",
              " 0.9777529,\n",
              " 0.9504417,\n",
              " 0.7234618,\n",
              " 0.9995442,\n",
              " 0.9995442,\n",
              " 0.72439057,\n",
              " 0.5365132,\n",
              " 0.99486625,\n",
              " 0.8772988,\n",
              " 0.98608714,\n",
              " 0.9203432,\n",
              " 0.68834114,\n",
              " 0.9984049,\n",
              " 0.8830958,\n",
              " 0.99475634,\n",
              " 0.80209476,\n",
              " 0.9020421,\n",
              " 0.9850454,\n",
              " 0.9850454,\n",
              " 0.5344501,\n",
              " 0.9867942,\n",
              " 0.74865365,\n",
              " 0.9884117,\n",
              " 0.9884117,\n",
              " 0.98421985,\n",
              " 0.7539735,\n",
              " 0.9792164,\n",
              " 0.58054274,\n",
              " 0.5827425,\n",
              " 0.5827425,\n",
              " 0.9765429,\n",
              " 0.99212486,\n",
              " 0.93177086,\n",
              " 0.99212486,\n",
              " 0.8506141,\n",
              " 0.9919585,\n",
              " 0.9998448,\n",
              " 0.81090546,\n",
              " 0.9341084,\n",
              " 0.8497939,\n",
              " 0.90583193,\n",
              " 0.60758555,\n",
              " 0.99008304,\n",
              " 0.7045565,\n",
              " 0.7045565,\n",
              " 0.7045565,\n",
              " 0.9532247,\n",
              " 0.9935588,\n",
              " 0.71161526,\n",
              " 0.826419,\n",
              " 0.96934724,\n",
              " 0.81246275,\n",
              " 0.7265027,\n",
              " 0.6931173,\n",
              " 0.719443,\n",
              " 0.9868868,\n",
              " 0.8079941,\n",
              " 0.732806,\n",
              " 0.99412113,\n",
              " 0.9693821,\n",
              " 0.7678956,\n",
              " 0.83884573,\n",
              " 0.8563046,\n",
              " 0.6837686,\n",
              " 0.9633956,\n",
              " 0.99205536,\n",
              " 0.92741215,\n",
              " 0.9976096,\n",
              " 0.9976096,\n",
              " 0.72115856,\n",
              " 0.64881164,\n",
              " 0.54532665,\n",
              " 0.54532665,\n",
              " 0.95174605,\n",
              " 0.95174605,\n",
              " 0.948482,\n",
              " 0.9720034,\n",
              " 0.9833572,\n",
              " 0.998985,\n",
              " 0.8730483,\n",
              " 0.9994367,\n",
              " 0.8595755,\n",
              " 0.9072678,\n",
              " 0.94535655,\n",
              " 0.8164374,\n",
              " 0.96682024,\n",
              " 0.97610223,\n",
              " 0.9990386,\n",
              " 0.9990386,\n",
              " 0.9990386,\n",
              " 0.54972565,\n",
              " 0.74886286,\n",
              " 0.74886286,\n",
              " 0.99885476,\n",
              " 0.74886286,\n",
              " 0.99883693,\n",
              " 0.88031244,\n",
              " 0.8169908,\n",
              " 0.9921456,\n",
              " 0.9921456,\n",
              " 0.9921456,\n",
              " 0.915792,\n",
              " 0.7135117,\n",
              " 0.7026322,\n",
              " 0.8131357,\n",
              " 0.9789828,\n",
              " 0.9323078,\n",
              " 0.78143215,\n",
              " 0.86252725,\n",
              " 0.97886676,\n",
              " 0.99758327,\n",
              " 0.99758327,\n",
              " 0.94768596,\n",
              " 0.98480725,\n",
              " 0.98480725,\n",
              " 0.9989672,\n",
              " 0.76567966,\n",
              " 0.7961915,\n",
              " 0.61882234,\n",
              " 0.89318633,\n",
              " 0.99755234,\n",
              " 0.99755234,\n",
              " 0.99755234,\n",
              " 0.893876,\n",
              " 0.96324956,\n",
              " 0.9995807,\n",
              " 0.988274,\n",
              " 0.9733015,\n",
              " 0.77944845,\n",
              " 0.9448098,\n",
              " 0.6035941,\n",
              " 0.9971615,\n",
              " 0.9410311,\n",
              " 0.91949135,\n",
              " 0.7118622,\n",
              " 0.6730789,\n",
              " 0.9312827,\n",
              " 0.96717894,\n",
              " 0.96717894,\n",
              " 0.7344905,\n",
              " 0.58047885,\n",
              " 0.9980698,\n",
              " 0.9980698,\n",
              " 0.91538626,\n",
              " 0.99799985,\n",
              " 0.99799985,\n",
              " 0.99799985,\n",
              " 0.9668286,\n",
              " 0.8106137,\n",
              " 0.94295543,\n",
              " 0.6310762,\n",
              " 0.8729017,\n",
              " 0.9611745,\n",
              " 0.9611745,\n",
              " 0.99776936,\n",
              " 0.872074,\n",
              " 0.8952474,\n",
              " 0.88507134,\n",
              " 0.5303598,\n",
              " 0.9030633,\n",
              " 0.9134427,\n",
              " 0.9041658,\n",
              " 0.9041658,\n",
              " 0.9050076,\n",
              " 0.92658037,\n",
              " 0.95357937,\n",
              " 0.904873,\n",
              " 0.9625744,\n",
              " 0.9717969,\n",
              " 0.7043477,\n",
              " 0.9993286,\n",
              " 0.98638606,\n",
              " 0.99704844,\n",
              " 0.99704844,\n",
              " 0.99704844,\n",
              " 0.99704844,\n",
              " 0.9726333,\n",
              " 0.9397007,\n",
              " 0.94288474,\n",
              " 0.7036286,\n",
              " 0.99409676,\n",
              " 0.77195215,\n",
              " 0.9204856,\n",
              " 0.98909587,\n",
              " 0.9636927,\n",
              " 0.89467055,\n",
              " 0.993097,\n",
              " 0.9318195,\n",
              " 0.9911384,\n",
              " 0.83674437,\n",
              " 0.69279355,\n",
              " 0.9679951,\n",
              " 0.5426098,\n",
              " 0.94700736,\n",
              " 0.9661885,\n",
              " 0.8558365,\n",
              " 0.99704176,\n",
              " 0.9524756,\n",
              " 0.9666305,\n",
              " 0.77746093,\n",
              " 0.88966155,\n",
              " 0.88966155,\n",
              " 0.87186134,\n",
              " 0.98468417,\n",
              " 0.66227615,\n",
              " 0.9364466,\n",
              " 0.76907706,\n",
              " 0.99970156,\n",
              " 0.99970156,\n",
              " 0.98281544,\n",
              " 0.98281544,\n",
              " 0.9751926,\n",
              " 0.9889637,\n",
              " 0.99071234,\n",
              " 0.54982996,\n",
              " 0.99962246,\n",
              " 0.9861859,\n",
              " 0.71519756,\n",
              " 0.9581312,\n",
              " 0.76625603,\n",
              " 0.9887505,\n",
              " 0.98881257,\n",
              " 0.94270587,\n",
              " 0.9809281,\n",
              " 0.7990025,\n",
              " 0.9914541,\n",
              " 0.9914541,\n",
              " 0.9976882,\n",
              " 0.9765976,\n",
              " 0.6296005,\n",
              " 0.9465739,\n",
              " 0.92792344,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKk9jEKP9-rE",
        "outputId": "9b3793cb-a986-4572-89d5-2bcbad8f7a9d"
      },
      "source": [
        "len(labels_to_eval)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1171"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD_9S_ANDrAo"
      },
      "source": [
        "dataset_test[\"predicted_label_pos\"] = labels_to_eval\n",
        "dataset_test[\"confidence_label_pos\"] = confidences"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxpF_Axq-F34",
        "outputId": "3771f0e7-a65b-4d50-bfe8-ed9f98e9702d"
      },
      "source": [
        "!pip install scikit-learn\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(dataset_test[\"pos\"],labels_to_eval,digits=5))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.92090   0.93202   0.92643       662\n",
            "           1    0.91018   0.89587   0.90297       509\n",
            "\n",
            "    accuracy                        0.91631      1171\n",
            "   macro avg    0.91554   0.91395   0.91470      1171\n",
            "weighted avg    0.91624   0.91631   0.91623      1171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFPdza90HcCF"
      },
      "source": [
        "dataset_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l346o4qL7HK"
      },
      "source": [
        "m.save(\"/content/drive/MyDrive/SI_AI_KBS/integrazione_AlBERTo_WMAL/sentipolc_agritrend/models/model_3_pos_abstita.h5\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_Pkf6QWMTTw"
      },
      "source": [
        "dataset_test.to_csv(\"/content/drive/MyDrive/SI_AI_KBS/integrazione_AlBERTo_WMAL/sentipolc_agritrend/models/predictions_AGRITREND_pos_full.csv\");"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}