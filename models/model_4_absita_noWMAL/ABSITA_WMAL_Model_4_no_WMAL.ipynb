{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ABSITA_WMAL_Model-4-no WMAL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-sf82movkvc",
        "outputId": "cc8ff1ba-6e38-4c90-d07d-c632e243a085"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLlrzHzRv9XU"
      },
      "source": [
        "import joblib\n",
        "emb_matrix_train = joblib.load(\"/content/drive/MyDrive/absita_comfort_embedd_128_768_alberto_train.joblib\")\n",
        "wmal_matrix_train = joblib.load(\"/content/drive/MyDrive/wmal_train_comfort_128.joblib\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se9351L01RCX"
      },
      "source": [
        "import pandas as pd\n",
        "dataset_DF = pd.read_csv(\"/content/drive/MyDrive/comfort_train_processed.csv\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2deOEjg1W_e",
        "outputId": "ae3b8186-a19c-4944-f090-7a69f537cc1e"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# define example\n",
        "data = dataset_DF[\"pos\"]\n",
        "values = array(data)\n",
        "print(values)\n",
        "# integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(values)\n",
        "print(integer_encoded)\n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "print(onehot_encoded)\n",
        "# invert first example\n",
        "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
        "print(inverted)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 1 ... 1 0 0]\n",
            "[0 1 1 ... 1 0 0]\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " ...\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Tc64TlXwys4",
        "outputId": "8e882aed-490d-42df-9fbc-3a53c5f9af50"
      },
      "source": [
        "from keras.layers import Input, Dense, LSTM, Bidirectional, TimeDistributed, Flatten, Concatenate, Dropout\n",
        "!pip install keras-self-attention\n",
        "from keras_self_attention import SeqWeightedAttention, SeqSelfAttention\n",
        "from keras.layers import Add, Multiply"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.7/dist-packages (0.51.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEm3BOiY8EHn"
      },
      "source": [
        "emb_matrix_test = joblib.load(\"/content/drive/MyDrive/absita_comfort_embedd_128_768_alberto_test.joblib\")\n",
        "dataset_test = pd.read_csv(\"/content/drive/MyDrive/comfort_test_processed.csv\")\n",
        "wmal_matrix_test = joblib.load(\"/content/drive/MyDrive/wmal_test_comfort_128.joblib\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ictcNZPP8TWc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6403761b-7e6a-4662-93a0-39db7ff08bf1"
      },
      "source": [
        "# define example\n",
        "data = dataset_test[\"pos\"]\n",
        "values = array(data)\n",
        "print(values)\n",
        "# integer encode\n",
        "integer_encoded_test = label_encoder.transform(values)\n",
        "print(integer_encoded_test)\n",
        "# binary encode\n",
        "integer_encoded_test = integer_encoded_test.reshape(len(integer_encoded_test), 1)\n",
        "onehot_encoded_test = onehot_encoder.transform(integer_encoded_test)\n",
        "print(onehot_encoded_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 ... 1 1 0]\n",
            "[0 0 0 ... 1 1 0]\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " ...\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaSsX9AewjPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ba4fa5a-7c3c-4304-ab1b-88425b4553ff"
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "import numpy as np\n",
        "\n",
        "input_emb = Input(shape=(128,768))\n",
        "#input_wmal = Input(shape=(128,))\n",
        "\n",
        "#Bi-LSTM\n",
        "#bi = Bidirectional(LSTM(64, return_sequences=True))(input_emb)\n",
        "bi = LSTM(64, return_sequences=True)(input_emb)\n",
        "att = SeqSelfAttention(attention_width=15,attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
        "                       attention_activation=\"sigmoid\",\n",
        "                       kernel_regularizer=keras.regularizers.l2(1e-8),#(1e-6),\n",
        "                       bias_regularizer=keras.regularizers.l1(1e-4),#(1e-4),\n",
        "                       attention_regularizer_weight=1e-8,#1e-4,\n",
        "                       use_attention_bias=True) (bi)\n",
        "\n",
        "app = Add() ([bi, att])                      \n",
        "flat = Flatten()(app)\n",
        "den1 = Dense(4000) (flat)\n",
        "den2 = Dense(2000) (den1)\n",
        "den3 = Dense(1000) (den2)\n",
        "den4 = Dense(500) (den3)\n",
        "den5 = Dense(256) (den4)\n",
        "den6 = Dense(128) (den5)\n",
        "den7 = Dense(64) (den6)\n",
        "\n",
        "#WMAL \n",
        "#d1 = Dense(64) (input_wmal)\n",
        "\n",
        "#Concat\n",
        "#concat = Concatenate(axis=1)([den7, d1])\n",
        "d1 = Dense(32, activation=\"relu\") (den7)\n",
        "output = Dense(2,activation=\"softmax\") (d1)\n",
        "\n",
        "m = keras.Model(inputs=[input_emb], outputs=output)\n",
        "\n",
        "opt = keras.optimizers.RMSprop(lr=0.0001)\n",
        "\n",
        "m.compile(loss=\"categorical_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])\n",
        "\n",
        "class myCallback(keras.callbacks.Callback):\n",
        "    bestF1=0\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._get_pred = None\n",
        "        self.preds = []\n",
        "    def on_epoch_end(self,epoch, logs=None):\n",
        "        predictions = m.predict([emb_matrix_test])\n",
        "        predictions_vec = np.zeros((1171,4))\n",
        "        i = 0\n",
        "        confidences= []\n",
        "        for item in predictions:\n",
        "          index = np.argmax(item);\n",
        "          confidences.append(item[index])\n",
        "          predictions_vec[i][index] = 1\n",
        "          i = i+1\n",
        "        labels_to_eval = []\n",
        "        for vec in predictions_vec:\n",
        "          inverted = label_encoder.inverse_transform([argmax(vec)])[0]\n",
        "          labels_to_eval.append(inverted)\n",
        "        \n",
        "        precision,recall,fscore,support=score(dataset_test[\"pos\"],labels_to_eval,average='macro')\n",
        "        print(\"Test f1-score:\"+str(fscore))\n",
        "        if fscore > self.bestF1:\n",
        "          m.save(\"bestModel.h5\")\n",
        "          print(\"BEST MODEL!!!!!\")\n",
        "          self.bestF1 = fscore\n",
        "\n",
        "callbacks = [myCallback()]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8V5UduPAvTH"
      },
      "source": [
        "m.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c__iZWCz32Tv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3938a190-9839-4796-d1f1-4718b1269689"
      },
      "source": [
        "m.fit([emb_matrix_train],onehot_encoded,batch_size=128, epochs=40,validation_split=0.15, callbacks=[callbacks])\n",
        "#m.load_weights(\"/content/drive/MyDrive/SI_AI_KBS/integrazione_AlBERTo_WMAL/sentipolc_agritrend/models/model_3_sentipolc/model_3_neg_sentipolc_0.72803.h5\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 1.3284 - accuracy: 0.7400Test f1-score:0.7711316087326868\n",
            "BEST MODEL!!!!!\n",
            "18/18 [==============================] - 17s 268ms/step - loss: 1.3284 - accuracy: 0.7400 - val_loss: 0.5157 - val_accuracy: 0.7706\n",
            "Epoch 2/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.3386 - accuracy: 0.8814Test f1-score:0.8112173475990285\n",
            "BEST MODEL!!!!!\n",
            "18/18 [==============================] - 4s 212ms/step - loss: 0.3386 - accuracy: 0.8814 - val_loss: 0.5271 - val_accuracy: 0.7706\n",
            "Epoch 3/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.2414 - accuracy: 0.9097Test f1-score:0.832527048874518\n",
            "BEST MODEL!!!!!\n",
            "18/18 [==============================] - 3s 192ms/step - loss: 0.2414 - accuracy: 0.9097 - val_loss: 0.3521 - val_accuracy: 0.8703\n",
            "Epoch 4/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1956 - accuracy: 0.9339Test f1-score:0.8502125202378472\n",
            "BEST MODEL!!!!!\n",
            "18/18 [==============================] - 3s 176ms/step - loss: 0.1956 - accuracy: 0.9339 - val_loss: 0.3711 - val_accuracy: 0.8678\n",
            "Epoch 5/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1995 - accuracy: 0.9277Test f1-score:0.8753711267941618\n",
            "BEST MODEL!!!!!\n",
            "18/18 [==============================] - 3s 172ms/step - loss: 0.1995 - accuracy: 0.9277 - val_loss: 0.3252 - val_accuracy: 0.8878\n",
            "Epoch 6/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1522 - accuracy: 0.9476Test f1-score:0.8800879286921972\n",
            "BEST MODEL!!!!!\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.1522 - accuracy: 0.9476 - val_loss: 0.4821 - val_accuracy: 0.8953\n",
            "Epoch 7/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0938 - accuracy: 0.9661Test f1-score:0.8945504340675083\n",
            "BEST MODEL!!!!!\n",
            "18/18 [==============================] - 3s 179ms/step - loss: 0.0938 - accuracy: 0.9661 - val_loss: 0.4212 - val_accuracy: 0.9002\n",
            "Epoch 8/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9647Test f1-score:0.86972755811649\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.1057 - accuracy: 0.9647 - val_loss: 0.5950 - val_accuracy: 0.8778\n",
            "Epoch 9/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9766Test f1-score:0.8920673148395921\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.0612 - accuracy: 0.9766 - val_loss: 0.4559 - val_accuracy: 0.9052\n",
            "Epoch 10/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0621 - accuracy: 0.9758Test f1-score:0.8911594433821968\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.0621 - accuracy: 0.9758 - val_loss: 0.5328 - val_accuracy: 0.8978\n",
            "Epoch 11/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.9819Test f1-score:0.851604613974803\n",
            "18/18 [==============================] - 2s 119ms/step - loss: 0.0572 - accuracy: 0.9819 - val_loss: 0.6211 - val_accuracy: 0.8529\n",
            "Epoch 12/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9771Test f1-score:0.8822793457770419\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.0644 - accuracy: 0.9771 - val_loss: 0.9410 - val_accuracy: 0.8778\n",
            "Epoch 13/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 0.9833Test f1-score:0.8785570806213991\n",
            "18/18 [==============================] - 2s 112ms/step - loss: 0.0596 - accuracy: 0.9833 - val_loss: 0.6699 - val_accuracy: 0.8903\n",
            "Epoch 14/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9784Test f1-score:0.8981358322732318\n",
            "BEST MODEL!!!!!\n",
            "18/18 [==============================] - 3s 171ms/step - loss: 0.0735 - accuracy: 0.9784 - val_loss: 0.7234 - val_accuracy: 0.9002\n",
            "Epoch 15/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9859Test f1-score:0.8857212884058521\n",
            "18/18 [==============================] - 2s 110ms/step - loss: 0.0377 - accuracy: 0.9859 - val_loss: 0.6491 - val_accuracy: 0.8978\n",
            "Epoch 16/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0900 - accuracy: 0.9855Test f1-score:0.8810043475187332\n",
            "18/18 [==============================] - 2s 107ms/step - loss: 0.0900 - accuracy: 0.9855 - val_loss: 0.6603 - val_accuracy: 0.9052\n",
            "Epoch 17/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9855Test f1-score:0.8934822366859267\n",
            "18/18 [==============================] - 2s 128ms/step - loss: 0.0407 - accuracy: 0.9855 - val_loss: 0.9564 - val_accuracy: 0.9152\n",
            "Epoch 18/40\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.0274 - accuracy: 0.9890Test f1-score:0.8810442909386429\n",
            "18/18 [==============================] - 2s 109ms/step - loss: 0.0271 - accuracy: 0.9890 - val_loss: 0.9920 - val_accuracy: 0.9102\n",
            "Epoch 19/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.9815Test f1-score:0.9015223061952764\n",
            "BEST MODEL!!!!!\n",
            "18/18 [==============================] - 3s 182ms/step - loss: 0.0400 - accuracy: 0.9815 - val_loss: 0.7365 - val_accuracy: 0.9152\n",
            "Epoch 20/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9855Test f1-score:0.892364940995078\n",
            "18/18 [==============================] - 2s 110ms/step - loss: 0.0469 - accuracy: 0.9855 - val_loss: 0.7201 - val_accuracy: 0.9152\n",
            "Epoch 21/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9881Test f1-score:0.9000189322929233\n",
            "18/18 [==============================] - 2s 122ms/step - loss: 0.0274 - accuracy: 0.9881 - val_loss: 0.8305 - val_accuracy: 0.9027\n",
            "Epoch 22/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9885Test f1-score:0.8887932620682697\n",
            "18/18 [==============================] - 2s 122ms/step - loss: 0.0285 - accuracy: 0.9885 - val_loss: 0.9582 - val_accuracy: 0.9127\n",
            "Epoch 23/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9824Test f1-score:0.8907241508025383\n",
            "18/18 [==============================] - 2s 107ms/step - loss: 0.0325 - accuracy: 0.9824 - val_loss: 1.0605 - val_accuracy: 0.8953\n",
            "Epoch 24/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9877Test f1-score:0.8984185992714686\n",
            "18/18 [==============================] - 2s 112ms/step - loss: 0.0316 - accuracy: 0.9877 - val_loss: 0.7422 - val_accuracy: 0.9127\n",
            "Epoch 25/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9894Test f1-score:0.888547365469818\n",
            "18/18 [==============================] - 2s 108ms/step - loss: 0.0197 - accuracy: 0.9894 - val_loss: 0.9179 - val_accuracy: 0.9152\n",
            "Epoch 26/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9885Test f1-score:0.9013918462112189\n",
            "18/18 [==============================] - 2s 108ms/step - loss: 0.0407 - accuracy: 0.9885 - val_loss: 1.0047 - val_accuracy: 0.8953\n",
            "Epoch 27/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9894Test f1-score:0.8962247429989365\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.0187 - accuracy: 0.9894 - val_loss: 1.1109 - val_accuracy: 0.9127\n",
            "Epoch 28/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9855Test f1-score:0.8874883962939093\n",
            "18/18 [==============================] - 2s 110ms/step - loss: 0.0266 - accuracy: 0.9855 - val_loss: 1.4000 - val_accuracy: 0.9152\n",
            "Epoch 29/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9890Test f1-score:0.882943787694233\n",
            "18/18 [==============================] - 2s 108ms/step - loss: 0.0196 - accuracy: 0.9890 - val_loss: 1.3252 - val_accuracy: 0.8978\n",
            "Epoch 30/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9802Test f1-score:0.8820758218938934\n",
            "18/18 [==============================] - 2s 107ms/step - loss: 0.0586 - accuracy: 0.9802 - val_loss: 1.0335 - val_accuracy: 0.8953\n",
            "Epoch 31/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9859Test f1-score:0.8956476794752977\n",
            "18/18 [==============================] - 2s 120ms/step - loss: 0.0384 - accuracy: 0.9859 - val_loss: 1.3768 - val_accuracy: 0.9102\n",
            "Epoch 32/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9872Test f1-score:0.8741897977805904\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.0432 - accuracy: 0.9872 - val_loss: 1.0202 - val_accuracy: 0.8953\n",
            "Epoch 33/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9894Test f1-score:0.8894722559788573\n",
            "18/18 [==============================] - 2s 107ms/step - loss: 0.0382 - accuracy: 0.9894 - val_loss: 1.1104 - val_accuracy: 0.8953\n",
            "Epoch 34/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9894Test f1-score:0.8031278712880965\n",
            "18/18 [==============================] - 2s 110ms/step - loss: 0.0282 - accuracy: 0.9894 - val_loss: 1.2907 - val_accuracy: 0.8155\n",
            "Epoch 35/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9850Test f1-score:0.8956476794752977\n",
            "18/18 [==============================] - 2s 107ms/step - loss: 0.0346 - accuracy: 0.9850 - val_loss: 1.0372 - val_accuracy: 0.9052\n",
            "Epoch 36/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9921Test f1-score:0.9060388124461352\n",
            "BEST MODEL!!!!!\n",
            "18/18 [==============================] - 3s 172ms/step - loss: 0.0215 - accuracy: 0.9921 - val_loss: 1.1307 - val_accuracy: 0.8978\n",
            "Epoch 37/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9828Test f1-score:0.8943697170810191\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.0388 - accuracy: 0.9828 - val_loss: 1.0809 - val_accuracy: 0.9102\n",
            "Epoch 38/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0419 - accuracy: 0.9872Test f1-score:0.8805749265137488\n",
            "18/18 [==============================] - 2s 110ms/step - loss: 0.0419 - accuracy: 0.9872 - val_loss: 0.8232 - val_accuracy: 0.8953\n",
            "Epoch 39/40\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9903Test f1-score:0.9063119445784122\n",
            "BEST MODEL!!!!!\n",
            "18/18 [==============================] - 3s 168ms/step - loss: 0.0188 - accuracy: 0.9903 - val_loss: 0.8070 - val_accuracy: 0.8978\n",
            "Epoch 40/40\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.0149 - accuracy: 0.9926Test f1-score:0.9010885930386949\n",
            "18/18 [==============================] - 2s 107ms/step - loss: 0.0143 - accuracy: 0.9929 - val_loss: 0.8420 - val_accuracy: 0.9127\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffa2fdc2510>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnGfTDKQ8tFG"
      },
      "source": [
        "m.load_weights(\"bestModel.h5\")\n",
        "predictions = m.predict([emb_matrix_test])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC7taZDs9Y-_"
      },
      "source": [
        "import numpy as np\n",
        "predictions_vec = np.zeros((1171,4))\n",
        "i = 0\n",
        "confidences= []\n",
        "for item in predictions:\n",
        "  index = np.argmax(item);\n",
        "  confidences.append(item[index])\n",
        "  predictions_vec[i][index] = 1\n",
        "  i = i+1\n",
        "labels_to_eval = []\n",
        "for vec in predictions_vec:\n",
        "  inverted = label_encoder.inverse_transform([argmax(vec)])[0]\n",
        "  labels_to_eval.append(inverted)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxpF_Axq-F34",
        "outputId": "89bf942e-b927-4b2f-b648-99c4707b692a"
      },
      "source": [
        "!pip install scikit-learn\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(dataset_test[\"pos\"],labels_to_eval,digits=5))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.90043   0.94260   0.92103       662\n",
            "           1    0.92050   0.86444   0.89159       509\n",
            "\n",
            "    accuracy                        0.90863      1171\n",
            "   macro avg    0.91047   0.90352   0.90631      1171\n",
            "weighted avg    0.90916   0.90863   0.90824      1171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l346o4qL7HK"
      },
      "source": [
        "m.save(\"/content/drive/MyDrive/SI_AI_KBS/integrazione_AlBERTo_WMAL/sentipolc_agritrend/models/model4_noWMAL_pos.h5\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_Pkf6QWMTTw"
      },
      "source": [
        "dataset_test.to_csv(\"/content/drive/MyDrive/SI_AI_KBS/integrazione_AlBERTo_WMAL/sentipolc_agritrend/models/predictions_creagritrend_neg_full_0.72649_noWMAL.csv\");"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}