{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ABSITA_WMAL_Model_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-sf82movkvc",
        "outputId": "2214cf9a-9d7c-4eb3-e35e-2128e78f63ce"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLlrzHzRv9XU"
      },
      "source": [
        "import joblib\n",
        "emb_matrix_train = joblib.load(\"/content/drive/MyDrive/absita_comfort_embedd_128_768_alberto_train.joblib\")\n",
        "wmal_matrix_train = joblib.load(\"/content/drive/MyDrive/wmal_train_comfort_128.joblib\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se9351L01RCX"
      },
      "source": [
        "import pandas as pd\n",
        "dataset_DF = pd.read_csv(\"/content/drive/MyDrive/comfort_train_processed.csv\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2deOEjg1W_e",
        "outputId": "6b64ca46-a193-42a4-a58a-8025ec63cf44"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# define example\n",
        "data = dataset_DF[\"pos\"]\n",
        "values = array(data)\n",
        "print(values)\n",
        "# integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(values)\n",
        "print(integer_encoded)\n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "print(onehot_encoded)\n",
        "# invert first example\n",
        "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
        "print(inverted)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 1 ... 1 0 0]\n",
            "[0 1 1 ... 1 0 0]\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " ...\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Tc64TlXwys4",
        "outputId": "50e83769-35a3-4b04-c2f1-92aa92a380d2"
      },
      "source": [
        "from keras.layers import Input, Dense, LSTM, Bidirectional, TimeDistributed, Flatten, Concatenate, Dropout\n",
        "!pip install keras-self-attention\n",
        "from keras_self_attention import SeqWeightedAttention, SeqSelfAttention\n",
        "from keras.layers import Add, Multiply"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.7/dist-packages (0.51.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEm3BOiY8EHn"
      },
      "source": [
        "emb_matrix_test = joblib.load(\"/content/drive/MyDrive/absita_comfort_embedd_128_768_alberto_test.joblib\")\n",
        "dataset_test = pd.read_csv(\"/content/drive/MyDrive/comfort_test_processed.csv\")\n",
        "wmal_matrix_test = joblib.load(\"/content/drive/MyDrive/wmal_test_comfort_128.joblib\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ictcNZPP8TWc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f9eaf14-8789-47f9-8683-c7802986d938"
      },
      "source": [
        "# define example\n",
        "data = dataset_test[\"pos\"]\n",
        "values = array(data)\n",
        "print(values)\n",
        "# integer encode\n",
        "integer_encoded_test = label_encoder.transform(values)\n",
        "print(integer_encoded_test)\n",
        "# binary encode\n",
        "integer_encoded_test = integer_encoded_test.reshape(len(integer_encoded_test), 1)\n",
        "onehot_encoded_test = onehot_encoder.transform(integer_encoded_test)\n",
        "print(onehot_encoded_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 ... 1 1 0]\n",
            "[0 0 0 ... 1 1 0]\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " ...\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaSsX9AewjPU"
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "import numpy as np\n",
        "\n",
        "input_emb = Input(shape=(128,768))\n",
        "input_wmal = Input(shape=(128,))\n",
        "\n",
        "#Bi-LSTM\n",
        "#bi = Bidirectional(LSTM(64, return_sequences=True))(input_emb)\n",
        "bi = LSTM(64, return_sequences=True)(input_emb)\n",
        "att = SeqSelfAttention(attention_width=15,attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
        "                       attention_activation=\"sigmoid\",\n",
        "                       kernel_regularizer=keras.regularizers.l2(1e-8),#(1e-6),\n",
        "                       bias_regularizer=keras.regularizers.l1(1e-4),#(1e-4),\n",
        "                       attention_regularizer_weight=1e-8,#1e-4,\n",
        "                       use_attention_bias=True) (bi)\n",
        "\n",
        "app = Add() ([bi, att])                      \n",
        "flat = Flatten()(app)\n",
        "den1 = Dense(4000) (flat)\n",
        "den2 = Dense(2000) (den1)\n",
        "den3 = Dense(1000) (den2)\n",
        "den4 = Dense(500) (den3)\n",
        "den5 = Dense(256) (den4)\n",
        "den6 = Dense(128) (den5)\n",
        "den7 = Dense(64) (den6)\n",
        "\n",
        "#WMAL \n",
        "d1 = Dense(64) (input_wmal)\n",
        "\n",
        "#Concat\n",
        "concat = Concatenate(axis=1)([den7, d1])\n",
        "d1 = Dense(32, activation=\"relu\") (concat)\n",
        "output = Dense(2,activation=\"softmax\") (d1)\n",
        "\n",
        "m = keras.Model(inputs=[input_emb,input_wmal], outputs=output)\n",
        "\n",
        "opt = keras.optimizers.RMSprop(learning_rate=0.00035)\n",
        "\n",
        "m.compile(loss=\"categorical_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])\n",
        "\n",
        "class myCallback(keras.callbacks.Callback):\n",
        "    bestF1=0\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._get_pred = None\n",
        "        self.preds = []\n",
        "    def on_epoch_end(self,epoch, logs=None):\n",
        "        predictions = m.predict([emb_matrix_test,wmal_matrix_test])\n",
        "        predictions_vec = np.zeros((1171,4))\n",
        "        i = 0\n",
        "        confidences= []\n",
        "        for item in predictions:\n",
        "          index = np.argmax(item);\n",
        "          confidences.append(item[index])\n",
        "          predictions_vec[i][index] = 1\n",
        "          i = i+1\n",
        "        labels_to_eval = []\n",
        "        for vec in predictions_vec:\n",
        "          inverted = label_encoder.inverse_transform([argmax(vec)])[0]\n",
        "          labels_to_eval.append(inverted)\n",
        "        \n",
        "        precision,recall,fscore,support=score(dataset_test[\"pos\"],labels_to_eval,average='macro')\n",
        "        print(\"Test f1-score:\"+str(fscore))\n",
        "        if fscore > self.bestF1:\n",
        "          m.save(\"bestModel.h5\")\n",
        "          print(\"BEST MODEL!!!!!\")\n",
        "          self.bestF1 = fscore\n",
        "\n",
        "callbacks = [myCallback()]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8V5UduPAvTH"
      },
      "source": [
        "m.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c__iZWCz32Tv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ca4681a-cb2a-4471-efb2-ebbc5fd64740"
      },
      "source": [
        "m.fit([emb_matrix_train,wmal_matrix_train],onehot_encoded,batch_size=128, epochs=100,callbacks=[callbacks])\n",
        "#m.load_weights(\"/content/drive/MyDrive/SI_AI_KBS/integrazione_AlBERTo_WMAL/sentipolc_agritrend/models/model_3_sentipolc/model_3_neg_sentipolc_0.72803.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 11.1131 - accuracy: 0.6674Test f1-score:0.8182175143822482\n",
            "BEST MODEL!!!!!\n",
            "21/21 [==============================] - 20s 191ms/step - loss: 11.1131 - accuracy: 0.6674\n",
            "Epoch 2/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.4382 - accuracy: 0.8318Test f1-score:0.7692057692323162\n",
            "21/21 [==============================] - 2s 114ms/step - loss: 0.4382 - accuracy: 0.8318\n",
            "Epoch 3/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.9825 - accuracy: 0.7693Test f1-score:0.81639570694198\n",
            "21/21 [==============================] - 2s 108ms/step - loss: 0.9825 - accuracy: 0.7693\n",
            "Epoch 4/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.3689 - accuracy: 0.8974Test f1-score:0.8841582199494719\n",
            "BEST MODEL!!!!!\n",
            "21/21 [==============================] - 3s 163ms/step - loss: 0.3689 - accuracy: 0.8974\n",
            "Epoch 5/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6659 - accuracy: 0.8427Test f1-score:0.8795118738938963\n",
            "21/21 [==============================] - 2s 102ms/step - loss: 0.6659 - accuracy: 0.8427\n",
            "Epoch 6/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.2586 - accuracy: 0.9071Test f1-score:0.874595839568113\n",
            "21/21 [==============================] - 2s 97ms/step - loss: 0.2586 - accuracy: 0.9071\n",
            "Epoch 7/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.2963 - accuracy: 0.9116Test f1-score:0.8571968854969374\n",
            "21/21 [==============================] - 2s 96ms/step - loss: 0.2963 - accuracy: 0.9116\n",
            "Epoch 8/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.2109 - accuracy: 0.9240Test f1-score:0.9103247070439562\n",
            "BEST MODEL!!!!!\n",
            "21/21 [==============================] - 3s 157ms/step - loss: 0.2109 - accuracy: 0.9240\n",
            "Epoch 9/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.3123 - accuracy: 0.9007Test f1-score:0.8874410501336698\n",
            "21/21 [==============================] - 2s 111ms/step - loss: 0.3123 - accuracy: 0.9007\n",
            "Epoch 10/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1462 - accuracy: 0.9596Test f1-score:0.8997521852752997\n",
            "21/21 [==============================] - 2s 107ms/step - loss: 0.1462 - accuracy: 0.9596\n",
            "Epoch 11/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.4559 - accuracy: 0.8921Test f1-score:0.8793991290018832\n",
            "21/21 [==============================] - 2s 108ms/step - loss: 0.4559 - accuracy: 0.8921\n",
            "Epoch 12/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0829 - accuracy: 0.9738Test f1-score:0.8928172398950949\n",
            "21/21 [==============================] - 2s 110ms/step - loss: 0.0829 - accuracy: 0.9738\n",
            "Epoch 13/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0814 - accuracy: 0.9813Test f1-score:0.8992188937493693\n",
            "21/21 [==============================] - 2s 110ms/step - loss: 0.0814 - accuracy: 0.9813\n",
            "Epoch 14/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6492 - accuracy: 0.8865Test f1-score:0.9054102568705237\n",
            "21/21 [==============================] - 2s 107ms/step - loss: 0.6492 - accuracy: 0.8865\n",
            "Epoch 15/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9820Test f1-score:0.8678509162849293\n",
            "21/21 [==============================] - 2s 98ms/step - loss: 0.0801 - accuracy: 0.9820\n",
            "Epoch 16/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1134 - accuracy: 0.9715Test f1-score:0.9035506768899005\n",
            "21/21 [==============================] - 2s 99ms/step - loss: 0.1134 - accuracy: 0.9715\n",
            "Epoch 17/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0978 - accuracy: 0.9749Test f1-score:0.8947037136948115\n",
            "21/21 [==============================] - 2s 100ms/step - loss: 0.0978 - accuracy: 0.9749\n",
            "Epoch 18/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9622Test f1-score:0.9062075893575627\n",
            "21/21 [==============================] - 2s 99ms/step - loss: 0.1309 - accuracy: 0.9622\n",
            "Epoch 19/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1173 - accuracy: 0.9824Test f1-score:0.9079607185284494\n",
            "21/21 [==============================] - 2s 98ms/step - loss: 0.1173 - accuracy: 0.9824\n",
            "Epoch 20/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1201 - accuracy: 0.9809Test f1-score:0.9014733246116766\n",
            "21/21 [==============================] - 2s 99ms/step - loss: 0.1201 - accuracy: 0.9809\n",
            "Epoch 21/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9787Test f1-score:0.9030690584765448\n",
            "21/21 [==============================] - 2s 100ms/step - loss: 0.0706 - accuracy: 0.9787\n",
            "Epoch 22/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9854Test f1-score:0.9078558758132073\n",
            "21/21 [==============================] - 2s 114ms/step - loss: 0.0316 - accuracy: 0.9854\n",
            "Epoch 23/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1469 - accuracy: 0.9757Test f1-score:0.9086703553577755\n",
            "21/21 [==============================] - 2s 101ms/step - loss: 0.1469 - accuracy: 0.9757\n",
            "Epoch 24/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9869Test f1-score:0.9067048294995521\n",
            "21/21 [==============================] - 2s 110ms/step - loss: 0.0248 - accuracy: 0.9869\n",
            "Epoch 25/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0910 - accuracy: 0.9843Test f1-score:0.7828069636849713\n",
            "21/21 [==============================] - 2s 97ms/step - loss: 0.0910 - accuracy: 0.9843\n",
            "Epoch 26/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1639 - accuracy: 0.9543Test f1-score:0.9110420949146484\n",
            "BEST MODEL!!!!!\n",
            "21/21 [==============================] - 3s 162ms/step - loss: 0.1639 - accuracy: 0.9543\n",
            "Epoch 27/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9899Test f1-score:0.909765032245827\n",
            "21/21 [==============================] - 2s 109ms/step - loss: 0.0175 - accuracy: 0.9899\n",
            "Epoch 28/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9903Test f1-score:0.889423984891407\n",
            "21/21 [==============================] - 2s 101ms/step - loss: 0.0266 - accuracy: 0.9903\n",
            "Epoch 29/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0587 - accuracy: 0.9835Test f1-score:0.9101961456566459\n",
            "21/21 [==============================] - 2s 98ms/step - loss: 0.0587 - accuracy: 0.9835\n",
            "Epoch 30/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9854Test f1-score:0.9066102588068337\n",
            "21/21 [==============================] - 2s 111ms/step - loss: 0.0315 - accuracy: 0.9854\n",
            "Epoch 31/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9831Test f1-score:0.8385299444931347\n",
            "21/21 [==============================] - 2s 115ms/step - loss: 0.0715 - accuracy: 0.9831\n",
            "Epoch 32/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0574 - accuracy: 0.9850Test f1-score:0.8975275132745988\n",
            "21/21 [==============================] - 2s 98ms/step - loss: 0.0574 - accuracy: 0.9850\n",
            "Epoch 33/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9880Test f1-score:0.8967649871496025\n",
            "21/21 [==============================] - 2s 97ms/step - loss: 0.0232 - accuracy: 0.9880\n",
            "Epoch 34/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0635 - accuracy: 0.9801Test f1-score:0.9028095873663904\n",
            "21/21 [==============================] - 2s 99ms/step - loss: 0.0635 - accuracy: 0.9801\n",
            "Epoch 35/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9899Test f1-score:0.8627359043488454\n",
            "21/21 [==============================] - 2s 98ms/step - loss: 0.0237 - accuracy: 0.9899\n",
            "Epoch 36/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1025 - accuracy: 0.9768Test f1-score:0.9016845678234953\n",
            "21/21 [==============================] - 2s 99ms/step - loss: 0.1025 - accuracy: 0.9768\n",
            "Epoch 37/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9865Test f1-score:0.9106155034304393\n",
            "21/21 [==============================] - 2s 99ms/step - loss: 0.0291 - accuracy: 0.9865\n",
            "Epoch 38/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9891Test f1-score:0.9175193754768336\n",
            "BEST MODEL!!!!!\n",
            "21/21 [==============================] - 3s 156ms/step - loss: 0.0379 - accuracy: 0.9891\n",
            "Epoch 39/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9835Test f1-score:0.8959738182100501\n",
            "21/21 [==============================] - 2s 100ms/step - loss: 0.0767 - accuracy: 0.9835\n",
            "Epoch 40/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9891Test f1-score:0.9037115997584007\n",
            "21/21 [==============================] - 2s 119ms/step - loss: 0.0241 - accuracy: 0.9891\n",
            "Epoch 41/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9888Test f1-score:0.9013918462112189\n",
            "21/21 [==============================] - 2s 98ms/step - loss: 0.0368 - accuracy: 0.9888\n",
            "Epoch 42/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9876Test f1-score:0.9064638533485323\n",
            "21/21 [==============================] - 2s 97ms/step - loss: 0.0370 - accuracy: 0.9876\n",
            "Epoch 43/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9888Test f1-score:0.9038668211046696\n",
            "21/21 [==============================] - 2s 111ms/step - loss: 0.0165 - accuracy: 0.9888\n",
            "Epoch 44/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9843Test f1-score:0.6524646720298894\n",
            "21/21 [==============================] - 2s 98ms/step - loss: 0.0241 - accuracy: 0.9843\n",
            "Epoch 45/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.4503 - accuracy: 0.9757Test f1-score:0.8956479316573928\n",
            "21/21 [==============================] - 2s 99ms/step - loss: 0.4503 - accuracy: 0.9757\n",
            "Epoch 46/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9861Test f1-score:0.9098636367372958\n",
            "21/21 [==============================] - 2s 98ms/step - loss: 0.0346 - accuracy: 0.9861\n",
            "Epoch 47/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0447 - accuracy: 0.9865Test f1-score:0.8874264324616881\n",
            "21/21 [==============================] - 2s 99ms/step - loss: 0.0447 - accuracy: 0.9865\n",
            "Epoch 48/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9895Test f1-score:0.9131212543381668\n",
            "21/21 [==============================] - 2s 100ms/step - loss: 0.0288 - accuracy: 0.9895\n",
            "Epoch 49/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9858Test f1-score:0.90310531106304\n",
            "21/21 [==============================] - 3s 124ms/step - loss: 0.0850 - accuracy: 0.9858\n",
            "Epoch 50/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9918Test f1-score:0.9036585949607185\n",
            "21/21 [==============================] - 2s 107ms/step - loss: 0.0305 - accuracy: 0.9918\n",
            "Epoch 51/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9906Test f1-score:0.9104517393580096\n",
            "21/21 [==============================] - 2s 100ms/step - loss: 0.0146 - accuracy: 0.9906\n",
            "Epoch 52/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.4909 - accuracy: 0.9315Test f1-score:0.7797730220492867\n",
            "21/21 [==============================] - 2s 97ms/step - loss: 0.4909 - accuracy: 0.9315\n",
            "Epoch 53/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1186 - accuracy: 0.9715Test f1-score:0.9061544810622999\n",
            "21/21 [==============================] - 2s 98ms/step - loss: 0.1186 - accuracy: 0.9715\n",
            "Epoch 54/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9910Test f1-score:0.909764372739209\n",
            "21/21 [==============================] - 2s 97ms/step - loss: 0.0206 - accuracy: 0.9910\n",
            "Epoch 55/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9910Test f1-score:0.8771513765263543\n",
            "21/21 [==============================] - 2s 100ms/step - loss: 0.0171 - accuracy: 0.9910\n",
            "Epoch 56/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9835Test f1-score:0.8103574555004377\n",
            "21/21 [==============================] - 2s 98ms/step - loss: 0.0679 - accuracy: 0.9835\n",
            "Epoch 57/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 0.9861Test f1-score:0.9047658485448773\n",
            "21/21 [==============================] - 2s 101ms/step - loss: 0.0596 - accuracy: 0.9861\n",
            "Epoch 58/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9906Test f1-score:0.9023207316709865\n",
            "21/21 [==============================] - 2s 99ms/step - loss: 0.0153 - accuracy: 0.9906\n",
            "Epoch 59/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9906Test f1-score:0.8991950726740161\n",
            "21/21 [==============================] - 2s 105ms/step - loss: 0.0229 - accuracy: 0.9906\n",
            "Epoch 60/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9899Test f1-score:0.9112104243350598\n",
            "21/21 [==============================] - 2s 98ms/step - loss: 0.0269 - accuracy: 0.9899\n",
            "Epoch 61/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0591 - accuracy: 0.9880Test f1-score:0.9090621266427719\n",
            "21/21 [==============================] - 2s 100ms/step - loss: 0.0591 - accuracy: 0.9880\n",
            "Epoch 62/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9910Test f1-score:0.9099605317280265\n",
            "21/21 [==============================] - 2s 98ms/step - loss: 0.0142 - accuracy: 0.9910\n",
            "Epoch 63/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9910Test f1-score:0.9099605317280265\n",
            "21/21 [==============================] - 2s 99ms/step - loss: 0.0140 - accuracy: 0.9910\n",
            "Epoch 64/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9903Test f1-score:0.7610232151141962\n",
            "21/21 [==============================] - 2s 99ms/step - loss: 0.0224 - accuracy: 0.9903\n",
            "Epoch 65/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.9783Test f1-score:0.9007198994616697\n",
            "21/21 [==============================] - 2s 99ms/step - loss: 0.1370 - accuracy: 0.9783\n",
            "Epoch 66/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9906Test f1-score:0.9033839977918058\n",
            "21/21 [==============================] - 2s 100ms/step - loss: 0.0144 - accuracy: 0.9906\n",
            "Epoch 67/100\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9910"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnGfTDKQ8tFG"
      },
      "source": [
        "m.load_weights(\"bestModel.h5\")\n",
        "predictions = m.predict([emb_matrix_test,wmal_matrix_test])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC7taZDs9Y-_"
      },
      "source": [
        "import numpy as np\n",
        "predictions_vec = np.zeros((1171,4))\n",
        "i = 0\n",
        "confidences= []\n",
        "for item in predictions:\n",
        "  index = np.argmax(item);\n",
        "  confidences.append(item[index])\n",
        "  predictions_vec[i][index] = 1\n",
        "  i = i+1\n",
        "labels_to_eval = []\n",
        "for vec in predictions_vec:\n",
        "  inverted = label_encoder.inverse_transform([argmax(vec)])[0]\n",
        "  labels_to_eval.append(inverted)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxpF_Axq-F34",
        "outputId": "65572d28-09d4-47d3-b25c-dfffbd9bc35d"
      },
      "source": [
        "!pip install scikit-learn\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(dataset_test[\"pos\"],labels_to_eval,digits=5))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.93020   0.92598   0.92808       662\n",
            "           1    0.90430   0.90963   0.90695       509\n",
            "\n",
            "    accuracy                        0.91887      1171\n",
            "   macro avg    0.91725   0.91780   0.91752      1171\n",
            "weighted avg    0.91894   0.91887   0.91890      1171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l346o4qL7HK"
      },
      "source": [
        "m.save(\"/content/drive/MyDrive/SI_AI_KBS/integrazione_AlBERTo_WMAL/sentipolc_agritrend/models/model4_absita_pos.h5\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_Pkf6QWMTTw"
      },
      "source": [
        "#dataset_test.to_csv(\"/content/drive/MyDrive/SI_AI_KBS/integrazione_AlBERTo_WMAL/sentipolc_agritrend/models/predictions_AGRITREND_pos_full_0.60653.csv\");"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}